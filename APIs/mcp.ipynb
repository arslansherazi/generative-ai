{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6f5feb",
   "metadata": {},
   "source": [
    "# MCP Server using Fast API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Voice-Based Emotional Wellness Journal (LLM + MCP)\n",
    "\n",
    "# Directory Structure:\n",
    "# voice_wellness_journal/\n",
    "# ├── backend/\n",
    "# │   ├── main.py              # FastAPI app entrypoint\n",
    "# │   ├── audio_utils.py       # Audio saving and preprocessing\n",
    "# │   ├── whisper_transcriber.py # WhisperX transcription\n",
    "# │   ├── emotion_analyzer.py  # Emotion detection using LLM\n",
    "# │   └── journal_generator.py # Journal prompts and suggestions\n",
    "# ├── mcp/\n",
    "# │   ├── mcp_server.py        # MCP server with tools and memory\n",
    "# │   └── tools.py             # Registered MCP tools\n",
    "# ├── frontend/ (React app, not in this code file)\n",
    "# └── requirements.txt\n",
    "\n",
    "# ------------------------------ backend/main.py ------------------------------\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from backend.audio_utils import save_audio\n",
    "from backend.whisper_transcriber import transcribe_audio\n",
    "from mcp.mcp_server import run_mcp_agent\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/upload-audio\")\n",
    "async def upload_audio(file: UploadFile = File(...)):\n",
    "    path = save_audio(file)\n",
    "    transcript = transcribe_audio(path)\n",
    "    response = run_mcp_agent(session_id=\"user1\", user_input=transcript)\n",
    "    return {\"transcript\": transcript, \"analysis\": response}\n",
    "\n",
    "\n",
    "# -------------------------- backend/audio_utils.py --------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastapi import UploadFile\n",
    "\n",
    "def save_audio(file: UploadFile) -> str:\n",
    "    upload_path = Path(\"uploads\")\n",
    "    upload_path.mkdir(exist_ok=True)\n",
    "    file_location = upload_path / file.filename\n",
    "    with open(file_location, \"wb\") as buffer:\n",
    "        buffer.write(file.file.read())\n",
    "    return str(file_location)\n",
    "\n",
    "\n",
    "# --------------------- backend/whisper_transcriber.py ----------------------\n",
    "import whisperx\n",
    "\n",
    "def transcribe_audio(path: str) -> str:\n",
    "    model = whisperx.load_model(\"medium\", device=\"cpu\")\n",
    "    audio = whisperx.load_audio(path)\n",
    "    result = model.transcribe(audio, batch_size=8)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "# --------------------- backend/emotion_analyzer.py -------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def analyze_emotion(text: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the emotion expressed in the following journal entry. Respond with a single word or short phrase (e.g., happy, anxious, content, frustrated).\n",
    "\n",
    "    Journal Entry:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# ------------------ backend/journal_generator.py --------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_journal_prompt(emotion: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    The user expressed the emotion: {emotion}.\n",
    "    Suggest a reflective journal prompt to help process this feeling.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# ------------------------------ mcp/tools.py ------------------------------\n",
    "from backend.emotion_analyzer import analyze_emotion\n",
    "from backend.journal_generator import generate_journal_prompt\n",
    "from mcp.mcp_utils import tool\n",
    "\n",
    "@tool(\"analyze_emotion\")\n",
    "def emotion_tool(context, transcript: str):\n",
    "    return analyze_emotion(transcript)\n",
    "\n",
    "@tool(\"generate_prompt\")\n",
    "def prompt_tool(context, emotion: str):\n",
    "    return generate_journal_prompt(emotion)\n",
    "\n",
    "\n",
    "# -------------------------- mcp/mcp_server.py -----------------------------\n",
    "from mcp.agent import MCPAgent\n",
    "from mcp.memory import MemoryBuffer\n",
    "from mcp.tools import emotion_tool, prompt_tool\n",
    "\n",
    "memory = MemoryBuffer()\n",
    "agent = MCPAgent(tools=[emotion_tool, prompt_tool], memory=memory)\n",
    "\n",
    "def run_mcp_agent(session_id: str, user_input: str):\n",
    "    return agent.run(session_id=session_id, user_input=user_input)\n",
    "\n",
    "\n",
    "# -------------------------- requirements.txt ------------------------------\n",
    "fastapi\n",
    "uvicorn\n",
    "openai\n",
    "whisperx\n",
    "torch\n",
    "tqdm\n",
    "mcp-protocol\n",
    "python-dotenv\n",
    "\n",
    "# ----------------------------- END OF FILE --------------------------------\n",
    "\n",
    "# ✅ Frontend idea (not included here):\n",
    "# - Mic recorder (React): Save and send audio\n",
    "# - Display journal entry, detected emotion, suggested prompt\n",
    "# - Add calendar view to show entries by date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5355ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcp/client.py\n",
    "\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self, api_url: str):\n",
    "        \"\"\"\n",
    "        Initialize the MCP Client with the URL of the FastAPI or MCP server.\n",
    "        Example: http://localhost:8000\n",
    "        \"\"\"\n",
    "        self.api_url = api_url.rstrip(\"/\")\n",
    "\n",
    "    def send_audio(self, audio_path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Sends an audio file to the backend for processing.\n",
    "        :param audio_path: Path to the local audio file (.wav/.mp3)\n",
    "        :return: Dictionary with transcript and analysis\n",
    "        \"\"\"\n",
    "        with open(audio_path, \"rb\") as f:\n",
    "            files = {\"file\": (audio_path, f, \"audio/mpeg\")}\n",
    "            response = requests.post(f\"{self.api_url}/upload-audio\", files=files)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "\n",
    "    def analyze_text(self, session_id: str, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Directly sends text to the MCP agent if transcription is already done.\n",
    "        :param session_id: Identifier for session memory tracking\n",
    "        :param text: Journal or spoken content\n",
    "        :return: Processed output\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_input\": text\n",
    "        }\n",
    "        response = requests.post(f\"{self.api_url}/mcp-analyze\", json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Request\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_input: str\n",
    "\n",
    "@app.post(\"/mcp-analyze\")\n",
    "async def analyze_with_text(payload: TextRequest):\n",
    "    result = run_mcp_agent(session_id=payload.session_id, user_input=payload.user_input)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_client.py\n",
    "from mcp.client import MCPClient\n",
    "\n",
    "client = MCPClient(api_url=\"http://localhost:8000\")\n",
    "\n",
    "# Example 1: Using audio file\n",
    "result = client.send_audio(\"sample_recording.wav\")\n",
    "print(\"Transcript:\", result[\"transcript\"])\n",
    "print(\"Analysis:\", result[\"analysis\"])\n",
    "\n",
    "# Example 2: Direct text analysis\n",
    "text = \"I'm feeling really anxious about my exams.\"\n",
    "text_response = client.analyze_text(session_id=\"test123\", text=text)\n",
    "print(\"LLM Response:\", text_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-XwelIRLU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
