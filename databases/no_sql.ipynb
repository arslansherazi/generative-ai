{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da35930",
   "metadata": {},
   "source": [
    "# NoSQL DBs in Generative AI\n",
    "\n",
    "**MongoDB:** Used for storing structured/unstructured metadata, user prompts, model outputs, and other app-level data.\n",
    "\n",
    "**Elasticsearch:** Used for semantic search or retrieval-augmented generation (RAG), where relevant context is retrieved and passed to the LLM to enhance response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0d6ec",
   "metadata": {},
   "source": [
    "## AI Knowledge Assistant\n",
    "\n",
    "ðŸ—£ï¸ Accepts a user query\n",
    "\n",
    "ðŸ” Uses an Elasticsearch retriever tool to fetch related docs\n",
    "\n",
    "ðŸ§¾ Optionally checks user history or context from MongoDB\n",
    "\n",
    "ðŸ¤– Uses LangChain Agent to decide how to answer\n",
    "\n",
    "ðŸ’¬ Generates a response using LLM\n",
    "\n",
    "ðŸ§  Stores chat in MongoDB\n",
    "\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "User Query\n",
    "\n",
    "   â†“\n",
    "\n",
    "LangChain Agent\n",
    "\n",
    "   â”œâ”€â”€ Tool 1: ElasticsearchRetrieverTool\n",
    "\n",
    "   â”œâ”€â”€ Tool 2: MongoDBChatHistoryTool\n",
    "\n",
    "   â†“\n",
    "\n",
    "LLM generates answer using both tools\n",
    "\n",
    "   â†“\n",
    "\n",
    "Answer is saved to MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "80b72c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:22.906713Z",
     "start_time": "2025-04-17T13:55:22.896755Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "120a9035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:24.514285Z",
     "start_time": "2025-04-17T13:55:24.509108Z"
    }
   },
   "source": [
    "import traceback\n",
    "\n",
    "# from langchain.agents import initialize_agent\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from pymongo import MongoClient\n",
    "from elasticsearch import Elasticsearch"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "99590461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:28:07.076146Z",
     "start_time": "2025-04-17T13:28:07.072947Z"
    }
   },
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c0043369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:28:07.085194Z",
     "start_time": "2025-04-17T13:28:07.082331Z"
    }
   },
   "source": [
    "INDEX_NAME = \"product_docs_vector\"\n",
    "\n",
    "documents = [\n",
    "    \"To reset your password, go to the settings page and click on 'Forgot Password'.\",\n",
    "    \"Our refund policy allows returns within 30 days of purchase, with the original receipt.\",\n",
    "    \"All electronics come with a one-year manufacturer warranty unless specified otherwise.\",\n",
    "    \"You can track your order using the tracking number sent to your email after shipping.\",\n",
    "    \"Contact our support team 24/7 via live chat or by calling our toll-free number.\",\n",
    "    \"Premium members receive free shipping and priority customer support.\",\n",
    "    \"Your data is encrypted and stored securely as per industry standards.\",\n",
    "    \"Multiple failed login attempts will temporarily lock your account for 15 minutes.\",\n",
    "    \"You can change your communication preferences from your profile settings.\",\n",
    "    \"Install our mobile app for a better experience and faster checkout.\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:27.518738Z",
     "start_time": "2025-04-17T13:55:27.490250Z"
    }
   },
   "cell_type": "code",
   "source": "embedding = OpenAIEmbeddings()",
   "id": "1306912912f14016",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:31.605324Z",
     "start_time": "2025-04-17T13:55:29.070587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorstore = ElasticsearchStore.from_texts(\n",
    "    texts=documents,\n",
    "    embedding=embedding,\n",
    "    index_name=INDEX_NAME,\n",
    "    es_connection=es,\n",
    ")"
   ],
   "id": "ce7d7bccb69d729f",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m vectorstore = \u001B[43mElasticsearchStore\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[43m=\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mINDEX_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mes_connection\u001B[49m\u001B[43m=\u001B[49m\u001B[43mes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:1158\u001B[39m, in \u001B[36mElasticsearchStore.from_texts\u001B[39m\u001B[34m(cls, texts, embedding, metadatas, bulk_kwargs, **kwargs)\u001B[39m\n\u001B[32m   1153\u001B[39m elasticsearchStore = ElasticsearchStore._create_cls_from_kwargs(\n\u001B[32m   1154\u001B[39m     embedding=embedding, **kwargs\n\u001B[32m   1155\u001B[39m )\n\u001B[32m   1157\u001B[39m \u001B[38;5;66;03m# Encode the provided texts and add them to the newly created index.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1158\u001B[39m \u001B[43melasticsearchStore\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1159\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbulk_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbulk_kwargs\u001B[49m\n\u001B[32m   1160\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m elasticsearchStore\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_community/vectorstores/elasticsearch.py:1047\u001B[39m, in \u001B[36mElasticsearchStore.add_texts\u001B[39m\u001B[34m(self, texts, metadatas, ids, refresh_indices, create_index_if_not_exists, bulk_kwargs, **kwargs)\u001B[39m\n\u001B[32m   1027\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001B[39;00m\n\u001B[32m   1028\u001B[39m \n\u001B[32m   1029\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1042\u001B[39m \u001B[33;03m    List of ids from adding the texts into the vectorstore.\u001B[39;00m\n\u001B[32m   1043\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1044\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.embedding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1045\u001B[39m     \u001B[38;5;66;03m# If no search_type requires inference, we use the provided\u001B[39;00m\n\u001B[32m   1046\u001B[39m     \u001B[38;5;66;03m# embedding function to embed the texts.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m     embeddings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1049\u001B[39m     \u001B[38;5;66;03m# the search_type doesn't require inference, so we don't need to\u001B[39;00m\n\u001B[32m   1050\u001B[39m     \u001B[38;5;66;03m# embed the texts.\u001B[39;00m\n\u001B[32m   1051\u001B[39m     embeddings = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:576\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    574\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    575\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:471\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m471\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    475\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/resources/embeddings.py:128\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    122\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    123\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    124\u001B[39m             ).tolist()\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1242\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1228\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1229\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1230\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1237\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1238\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1239\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1240\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1241\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1242\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:919\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[39m\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    917\u001B[39m     retries_taken = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m919\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    922\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    923\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1008\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_retry(err.response):\n\u001B[32m   1007\u001B[39m     err.response.close()\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1017\u001B[39m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[32m   1018\u001B[39m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[32m   1019\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err.response.is_closed:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1057\u001B[39m, in \u001B[36mSyncAPIClient._retry_request\u001B[39m\u001B[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[39m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[32m   1055\u001B[39m time.sleep(timeout)\n\u001B[32m-> \u001B[39m\u001B[32m1057\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1061\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1062\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1008\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_retry(err.response):\n\u001B[32m   1007\u001B[39m     err.response.close()\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1017\u001B[39m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[32m   1018\u001B[39m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[32m   1019\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err.response.is_closed:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1057\u001B[39m, in \u001B[36mSyncAPIClient._retry_request\u001B[39m\u001B[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[39m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[32m   1055\u001B[39m time.sleep(timeout)\n\u001B[32m-> \u001B[39m\u001B[32m1057\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1061\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1062\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/openai/_base_client.py:1023\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1020\u001B[39m         err.response.read()\n\u001B[32m   1022\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1023\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_response(\n\u001B[32m   1026\u001B[39m     cast_to=cast_to,\n\u001B[32m   1027\u001B[39m     options=options,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1031\u001B[39m     retries_taken=retries_taken,\n\u001B[32m   1032\u001B[39m )\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f9dcbb3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:52.644060Z",
     "start_time": "2025-04-17T13:55:52.616315Z"
    }
   },
   "source": [
    "mongo = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = mongo[\"support_bot\"]\n",
    "chat_collection = db[\"chats\"]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "4757ffdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:55:55.295900Z",
     "start_time": "2025-04-17T13:55:55.213225Z"
    }
   },
   "source": "llm = ChatOpenAI(model=\"gpt-4\")",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m llm = \u001B[43mChatOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt-4\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_core/load/serializable.py:130\u001B[39m, in \u001B[36mSerializable.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[31m[... skipping hidden 1 frame]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_core/language_models/base.py:94\u001B[39m, in \u001B[36m_get_verbosity\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     91\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_verbosity\u001B[39m() -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m     92\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mglobals\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_verbose\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_verbose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/virtualenvs/generative-ai-XwelIRLU/lib/python3.12/site-packages/langchain_core/globals.py:85\u001B[39m, in \u001B[36mget_verbose\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     66\u001B[39m         warnings.filterwarnings(\n\u001B[32m     67\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     68\u001B[39m             message=(\n\u001B[32m   (...)\u001B[39m\u001B[32m     71\u001B[39m             ),\n\u001B[32m     72\u001B[39m         )\n\u001B[32m     73\u001B[39m         \u001B[38;5;66;03m# N.B.: This is a workaround for an unfortunate quirk of Python's\u001B[39;00m\n\u001B[32m     74\u001B[39m         \u001B[38;5;66;03m#       module-level `__getattr__()` implementation:\u001B[39;00m\n\u001B[32m     75\u001B[39m         \u001B[38;5;66;03m# https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     83\u001B[39m         \u001B[38;5;66;03m# deprecation warnings directing them to use `set_verbose()` when they\u001B[39;00m\n\u001B[32m     84\u001B[39m         \u001B[38;5;66;03m# import `langchain.verbose`.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m         old_verbose = \u001B[43mlangchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[32m     87\u001B[39m     old_verbose = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: module 'langchain' has no attribute 'verbose'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "c8a7078a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:57:07.436840Z",
     "start_time": "2025-04-17T13:57:07.429634Z"
    }
   },
   "source": [
    "# ==== Tool 1: Hybrid Elasticsearch Retriever ====\n",
    "class HybridESRetrieverTool(Tool):\n",
    "    name: str = \"Hybrid Elastic Search Retriever\"\n",
    "    description: str = \"Retrieves product knowledge using hybrid vector + text search.\"\n",
    "\n",
    "    def __init__(self, es_client: Elasticsearch, index_name: str):\n",
    "        super().__init__(self.name, self._preform_action, self.description)\n",
    "        self.es = es_client\n",
    "        self.index = index_name\n",
    "        self.embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "    def _preform_action(self, query: str) -> str | None:\n",
    "        try:\n",
    "            # Create embedding for the user query\n",
    "            query_vector = self.embedding_model.embed_query(query)\n",
    "\n",
    "            # Build hybrid search: text + vector\n",
    "            search_body = {\n",
    "                \"size\": 5,\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            {\n",
    "                                \"match\": {\n",
    "                                    \"content\": {\n",
    "                                        \"query\": query,\n",
    "                                        \"boost\": 1.0\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"knn\": {\n",
    "                                    \"embedding_vector\": {\n",
    "                                        \"vector\": query_vector,\n",
    "                                        \"k\": 5,\n",
    "                                        \"num_candidates\": 100\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            res = self.es.search(index=self.index, body=search_body)\n",
    "            hits = res.get(\"hits\", {}).get(\"hits\", [])\n",
    "\n",
    "            if not hits:\n",
    "                return \"No relevant content found.\"\n",
    "\n",
    "            results = []\n",
    "            for hit in hits:\n",
    "                source = hit.get(\"_source\", {})\n",
    "                content = source.get(\"content\", \"[No content found]\")\n",
    "                score = hit.get(\"_score\", 0)\n",
    "                results.append(f\"Score: {score:.2f}\\nContent:\\n{content}\")\n",
    "\n",
    "            return \"\\n\\n---\\n\\n\".join(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error during hybrid search: {str(e)}\\n{traceback.format_exc()}\""
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "6d269ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:56:58.306497Z",
     "start_time": "2025-04-17T13:56:58.300653Z"
    }
   },
   "source": [
    "# ==== Tool 2: MongoDB Chat History ====\n",
    "class MongoChatHistoryTool(Tool):\n",
    "    name: str = \"User Chat History Tool\"\n",
    "    description: str = \"useful for checking last 3 interactions by a user\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize tool.\"\"\"\n",
    "        super().__init__(name=self.name, description=self.description, func=self._preform_action)\n",
    "\n",
    "    @staticmethod\n",
    "    def _preform_action(user_id: str):\n",
    "        history = chat_collection.find({\"user_id\": user_id}).sort(\"timestamp\", -1).limit(3)\n",
    "        return \"\\n\".join([f\"User: {h['prompt']}\\nBot: {h['response']}\" for h in history])"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_tool = HybridESRetrieverTool(es_client=es, index_name=\"product_docs\")\n",
    "mongo_tool = MongoChatHistoryTool()\n",
    "\n",
    "\n",
    "tools = [\n",
    "    es_tool,\n",
    "    mongo_tool\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "20298b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T13:57:15.835010Z",
     "start_time": "2025-04-17T13:57:15.831356Z"
    }
   },
   "source": [
    "def generate_response(user_id, prompt):\n",
    "    system_prompt = f\"The user ID is {user_id}. The user asked: '{prompt}'\"\n",
    "    final_response = agent.run(system_prompt)\n",
    "\n",
    "    # Save to MongoDB\n",
    "    chat_collection.insert_one({\n",
    "        \"user_id\": user_id,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": final_response\n",
    "    })\n",
    "\n",
    "    return final_response"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb650fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(user_id=\"123\", prompt=\"How do I reset my password?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-XwelIRLU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
