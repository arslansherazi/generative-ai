{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Open AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "training_file_path = \"../../static/json/sales_policy_fine_tune_data.jsonl\"\n",
    "\n",
    "with open(training_file_path, \"r\") as file:\n",
    "    training_data = file.readlines()\n",
    "\n",
    "# Convert the file into the right format for OpenAI\n",
    "response = client.files.create(\n",
    "  file=open(training_file_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "training_file_id = response.id\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tuning_response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    hyperparameters={\n",
    "    \"n_epochs\":2\n",
    "  }\n",
    ")\n",
    "\n",
    "print(\"fine tuned model id: \", fine_tuning_response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.fine_tuning.jobs.retrieve(os.getenv(\"FINE_TUNED_MODEL_ID\"))\n",
    "print(\"Status: \", result.status)\n",
    "print(\"Error: \", result.error.message)\n",
    "print(\"Model: \", result.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the company policy on product returns?\n",
      "Answer: Customers can return products within 30 days of purchase with a receipt for a full refund.\n",
      "\n",
      "Question: Are there any conditions for availing discounts on bulk orders?\n",
      "Answer: Yes, some common conditions for availing discounts on bulk orders include:\n",
      "\n",
      "1. Minimum order quantity: A minimum quantity of products must be ordered to qualify for the discount.\n",
      "2. Order value: The total value of the order must meet a certain threshold to qualify for the discount.\n",
      "3. Purchase frequency: Discounts may be offered for recurring or frequent orders.\n",
      "4. Eligible products: The discount may only apply to specific products or categories.\n",
      "5. Payment terms: Discounts may be available for specific payment methods or terms.\n",
      "6. Time frame: The discount may only be available within a certain period or for a limited time.\n",
      "\n",
      "Question: How can customers file a warranty claim?\n",
      "Answer: Customers can file a warranty claim by contacting the manufacturer or retailer where they purchased the product. They may need to provide proof of purchase and information about the issue with the product.\n",
      "\n",
      "Question: What are the shipping charges for international orders?\n",
      "Answer: The shipping charges for international orders vary depending on the destination country and the weight of the package.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "queries = [\n",
    "    \"What is the company policy on product returns?\",\n",
    "    \"Are there any conditions for availing discounts on bulk orders?\",\n",
    "    \"How can customers file a warranty claim?\",\n",
    "    \"What are the shipping charges for international orders?\"\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=os.getenv(\"FINE_TUNED_MODEL\"))\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    messages = [\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    result = llm.invoke(messages)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {result.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-XwelIRLU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
