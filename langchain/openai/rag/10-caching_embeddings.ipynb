{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb89fdc",
   "metadata": {},
   "source": [
    "# Caching Embeddings\n",
    "In **Retrieval-Augmented Generation (RAG)** systems, text data is converted into vector embeddings for semantic search and retrieval.\n",
    "However, generating embeddings repeatedly for the same text is costly and time-consuming, especially when using APIs such as OpenAI or Azure.\n",
    "\n",
    "**Caching embeddings:** storing previously computed embeddings locally (or in a database) so that future queries for the same text can reuse them instantly instead of recomputing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07d08b",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "Consider a **customer support chatbot** that answers queries from customers about company policies, refunds, shipping, or product details.\n",
    "Many customers ask the same or similar questions repeatedly ‚Äî for example:\n",
    "\n",
    "- ‚ÄúWhat‚Äôs the refund policy?‚Äù\n",
    "\n",
    "- ‚ÄúHow can I return an item?‚Äù\n",
    "\n",
    "- ‚ÄúDo you offer free shipping?‚Äù\n",
    "\n",
    "In such cases, the chatbot needs to generate embeddings for these queries before performing a semantic search.\n",
    "Without caching, the same embeddings are generated multiple times for identical queries, leading to unnecessary API calls and higher latency.\n",
    "\n",
    "By implementing **embedding caching**, the system first checks whether an embedding for the given query already exists in the cache (e.g., OpenSearch, Redis, or local JSON).\n",
    "If found, it reuses that stored embedding instead of generating a new one ‚Äî resulting in:\n",
    "\n",
    "- Faster response times\n",
    "\n",
    "- Lower embedding API costs\n",
    "\n",
    "- Reduced computational overhead\n",
    "\n",
    "‚öôÔ∏è **When It‚Äôs Useful**\n",
    "\n",
    "- Chatbots that handle frequent, repetitive customer questions.\n",
    "\n",
    "- Scenarios with high query volume and limited variation in phrasing.\n",
    "\n",
    "- Deployments that scale horizontally, where multiple instances can reuse the same cached embeddings from a shared store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb6c3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a7b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77b6d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Warranty covers accidental screen damage for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Refunds are processed within 7 business days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free shipping is available for orders above $50.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Warranty covers accidental screen damage for o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Warranty covers accidental screen damage for o...\n",
       "1      Refunds are processed within 7 business days.\n",
       "2   Free shipping is available for orders above $50.\n",
       "3  Warranty covers accidental screen damage for o..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dataset: customer service knowledge base\n",
    "data = [\n",
    "    \"Warranty covers accidental screen damage for one year.\",\n",
    "    \"Refunds are processed within 7 business days.\",\n",
    "    \"Free shipping is available for orders above $50.\",\n",
    "    \"Warranty covers accidental screen damage for one year.\"  # Duplicate\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\"text\": data})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a425a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a consistent hash for each text string.\n",
    "\n",
    "    :param text: text to be embedded\n",
    "    \"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e27444",
   "metadata": {},
   "source": [
    "#### Caching Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31cf050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE = \"embedding_cache.json\"\n",
    "\n",
    "# Load existing cache if available\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        embedding_cache = json.load(f)\n",
    "else:\n",
    "    embedding_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6aae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_embedding(text):\n",
    "    \"\"\"Retrieve embedding from cache or generate if missing.\"\"\"\n",
    "    key = generate_hash(text)\n",
    "    if key in embedding_cache:\n",
    "        return embedding_cache[key]\n",
    "    else:\n",
    "        vector = embeddings.embed_query(text)\n",
    "        embedding_cache[key] = vector\n",
    "        return vector\n",
    "\n",
    "# Compute embeddings for dataset\n",
    "df[\"embedding\"] = df[\"text\"].apply(get_or_create_embedding)\n",
    "\n",
    "# Save updated cache\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(embedding_cache, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7566b",
   "metadata": {},
   "source": [
    "#### Vector Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72749106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index built successfully with caching enabled.\n",
      "üìä Index contains 4 documents\n"
     ]
    }
   ],
   "source": [
    "# Build a FAISS index using cached embeddings\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "vectors = df[\"embedding\"].tolist()\n",
    "\n",
    "# Create text-embedding pairs for FAISS.from_embeddings\n",
    "text_embeddings = list(zip(texts, vectors))\n",
    "\n",
    "vectorstore = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embeddings,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FAISS index built successfully with caching enabled.\")\n",
    "print(f\"üìä Index contains {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9047c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing query: 'What is the policy for free shipping?'\n",
      "\n",
      "1Ô∏è‚É£ First time (new embedding):\n",
      "   Time: 1.02 seconds\n",
      "2Ô∏è‚É£ Second time (cached embedding):\n",
      "   Time: 0.07 ms\n",
      "\n",
      "üìà Results:\n",
      "   Time saved: 1.02 seconds\n",
      "   Speed improvement: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def format_time(ms):\n",
    "    \"\"\"Format time in appropriate units (ms, seconds, or minutes).\"\"\"\n",
    "    if ms < 1000:\n",
    "        return f\"{ms:.2f} ms\"\n",
    "    elif ms < 60000:  # Less than 1 minute\n",
    "        seconds = ms / 1000\n",
    "        return f\"{seconds:.2f} seconds\"\n",
    "    else:\n",
    "        minutes = ms / 60000\n",
    "        return f\"{minutes:.2f} minutes\"\n",
    "\n",
    "# Test query\n",
    "query = \"What is the policy for free shipping?\"\n",
    "\n",
    "print(f\"üîç Testing query: '{query}'\")\n",
    "print()\n",
    "\n",
    "# First time - generate new embedding\n",
    "print(\"1Ô∏è‚É£ First time (new embedding):\")\n",
    "start_time = time.time()\n",
    "embedding1 = get_or_create_embedding(query)\n",
    "time1 = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {format_time(time1)}\")\n",
    "\n",
    "# Second time - use cached embedding\n",
    "print(\"2Ô∏è‚É£ Second time (cached embedding):\")\n",
    "start_time = time.time()\n",
    "embedding2 = get_or_create_embedding(query)\n",
    "time2 = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {format_time(time2)}\")\n",
    "\n",
    "# Show the difference\n",
    "time_saved = time1 - time2\n",
    "print(f\"\\nüìà Results:\")\n",
    "print(f\"   Time saved: {format_time(time_saved)}\")\n",
    "print(f\"   Speed improvement: {((time1 - time2) / time1) * 100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
